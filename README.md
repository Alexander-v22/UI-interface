# Gesture-Controlled UI Library & Smart UX Extensions

A modular **Svelte-based UI component and interaction system** showcasing **hand gesture recognition** and **face-distance-aware interactions**. Designed to enable intelligent, hands-free control for futuristic interfaces, accessibility use cases, and creative web experiences.

---

## üìå Project Origin

This project originally started during my time with the **Duke Code+ 2025** program. You can find the official GitLab version [here](https://gitlab.oit.duke.edu/codeplus/ui-inputs), but this version represents my own personal take with continued improvements and added experimental features.

---


## Features

### Gesture-Controlled UI Logic

- Powered by **TensorFlow MediaPipe Hands**
- Detects:
  - Index-based cursor movement
  - Fist-to-open drag-and-drop
  - Press-and-hold gestures
  - ‚ÄúOkay‚Äù gesture click (index + thumb pinch)
  - Scroll gestures (vertical flicks)
- Built-in **physics engine** for draggable objects:
  - Gravity, bounce, and inertia
- Rich UI feedback:
  - Hover, click, and press animations
  - Canvas-based visual overlays

### Face Distance Awareness Module

- Detects user‚Äôs distance from the webcam
- Can adjust UI size or trigger logic dynamically
- Useful for:
  - Accessibility
  - Engagement detection
  - Context-aware resizing or interface modulation

---

## Live Demo

[Try the Interactive Demo Here](https://avm-page.vercel.app/)  

Showcases gesture input combined with a physics engine for smooth, interactive web control.

---
## Author
Alexander Valdovinos Mena.

